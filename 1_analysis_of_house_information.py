# -*- coding: utf-8 -*-
"""1_Analysis of house information.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QzWbnbg9sIukUDIrp73QXnhfUkuSdVYf
"""

from google.colab import drive

drive.mount('/content/drive')

"""#[Problem 1] Obtaining a dataset

*Use pd.read_csv () to store it in a variable.*
"""

import pandas as pd

data = pd.read_csv('/content/drive/MyDrive/empasoftAIdata/train.csv')

data.head(5)

"""#[Problem 2] Investigating the dataset itself

*Please explain what kind of dataset it is by reading Kaggle's Overview page and "Data fields" on the Data page.*
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import itertools
import matplotlib.pyplot as plt
import sklearn as sk
import seaborn as sns
import missingno as msno
from scipy import stats
# %matplotlib inline
sns.set()

print(data.shape)

data.duplicated().sum()

data.info ()

data

"""#[Problem 3] Checking the data**

Let's check the data. Please report using a combination of code and markdown.

See what each feature is about. (Numerical data or categorical data, etc.)

Check which column is the target variable this time.

Display the mean, standard deviation, and quartiles of the feature values at once.
"""

#See what each feature is about. (Numerical data or text data, etc.)

numerical_feats = data.dtypes[data.dtypes != "object"].index
print("Number of Numerical data: ", len(numerical_feats))

categorical_feats = data.dtypes[data.dtypes == "object"].index
print("Number of text data: ", len(categorical_feats))

#See what each feature is about. (Numerical data or text data, etc.)

print(data[numerical_feats].columns)

print(data[categorical_feats].columns)

data[numerical_feats].head()

data[categorical_feats].head()

#Check which column is the target variable this time
sns.displot(data=data, x='SalePrice', kde=True, height=6)

data.describe()
# Display the mean, standard deviation, and quartiles of the feature values at once.

"""#[Problem 4] Dealing with missing values

Do the following for missing values and report back.

1.   Check for missing values for each feature. (To check for missing values, we can use a library called missingno. This library allows you to visualize the presence of missing values. For installation and usage, please refer to the following page)
2.   Check the percentage of missing values
3.   Delete features (columns) that have 5 or more missing values.
4.   Samples (rows) with missing values are deleted from the data from which features with 5 or more missing values have been deleted.











"""

#Check for missing values for each feature.
data.isnull().sum()

#Check the percentage of missing values
total = data.isnull().sum().sort_values(ascending=False)
percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)

"""#Missingno

https://github.com/ResidentMario/missingno
"""

# Commented out IPython magic to ensure Python compatibility.
# Delete features (columns) that have 5 or more missing values.
# %matplotlib inline
msno.matrix(data.sample(100))
msno.heatmap(data)

data.shape

df=data.drop(['Id','PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage', 'GarageYrBlt', 'GarageCond','GarageType','GarageFinish','GarageQual','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1','MasVnrArea','MasVnrType'], axis = 1)

df.shape

#Check the percentage of missing values
total = df.isnull().sum().sort_values(ascending=False)
percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)

df.info()

"""#[Problem 5] Researching terminology
You will need to know the following two words. Please research and summarize these.

•	kurtosis

•	skewness
"""

sns.distplot(df['SalePrice']);
#skewness and kurtosis
print("Skewness: %f" % df['SalePrice'].skew())
print("Kurtosis: %f" % df['SalePrice'].kurt())

"""#[Problem 6] Confirming distribution

•	Use seaborn's sns.displot() and sns.histplot() to display the distribution of objective variables and calculate "kurtosis" and "skewness".

•	Perform a logarithmic transformation on the objective variable.

•	The distribution is displayed for the logarithmically transformed one, and the "kurtosis" and "skewness" are also calculated.

"""

df['SalePrice_Log'] = np.log(df['SalePrice'])

sns.distplot(df['SalePrice_Log']);
# skewness and kurtosis
print("Skewness: %f" % df['SalePrice_Log'].skew())
print("Kurtosis: %f" % df['SalePrice_Log'].kurt())
# dropping old column
df.drop('SalePrice', axis= 1, inplace=True)

"""[Problem 7] Confirming the correlation coefficient

We will check the features. For each one, please write down a markdown description and discussion.
•	Create a heat map of the correlation coefficient matrix and find the relationships with high correlation coefficients.
•	Select 10 features that have a high correlation with the target variable, and create a correlation coefficient matrix heat map for these.
•	Summarize whether the 10 selected features represent something by referring to the description in Kaggle's DataDescription.
•	Find 3 combinations of the 10 selected features that have high correlation coefficients with each other.

"""

#correlation matrix
corrmat = df.corr()
f, ax = plt.subplots(figsize=(16, 8))
sns.heatmap(corrmat, vmax=.8, square=True);

#saleprice correlation matrix
k = 10 #number of variables for heatmap
cols = corrmat.nlargest(k, 'SalePrice_Log')['SalePrice_Log'].index
cm = np.corrcoef(df[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()
